{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os.path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup or get the url where you want to extract the data\n",
    "url = 'https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/subspace-basis/v/linear-subspaces'\n",
    "\n",
    "# Check the response from that url\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:   # 200 mean positive response if not then\n",
    "    print(\"Data Fetiching Error\")\n",
    "else:\n",
    "    print(response.status_code)   # Return the responce\n",
    "    print(response.text)        # Return all the test from that page\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Now parse the HTML content to BeautifulSoup\n",
    "    # soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")     # Instead of html.parser give lxml\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetiching HTML attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Find element by their tag name\n",
    "title = soup.title\n",
    "print(title)\n",
    "### Output: <title>Khan Academy</title>\n",
    "\n",
    "# Now extract the string from it \n",
    "print(title.string)\n",
    "\n",
    "div = soup.div\n",
    "# print(div)    # This ?will give you list of all di\n",
    "\n",
    "header = soup.header\n",
    "print(header)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:   # 200 mean positive response if not then\n",
    "    print(\"Data Fetiching Error\")\n",
    "else:\n",
    "    print(response.status_code)   # Return the responce\n",
    "    # print(response.text)        # Return all the test from that page\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")     # Instead of html.parser give lxml\n",
    "    # print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.title.string)\n",
    "# print(soup.div)\n",
    "\n",
    "# Print how many links\n",
    "links = len(soup.find_all(\"a\"))\n",
    "print(links)\n",
    "\n",
    "# Get the first link\n",
    "print(soup.find_all(\"a\")[1])\n",
    "\n",
    "span = soup.find_all(\"span\", class_=\"text\")\n",
    "print(span[1].get_text())\n",
    "print()\n",
    "\n",
    "# Find the attribute \n",
    "div = soup.find(\"div\", class_=\"row header-box\")\n",
    "print(div.attrs)\n",
    "\n",
    "print(span)\n",
    "\n",
    "# Print All the span texts\n",
    "for i in span:\n",
    "    print(i.get_text())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Multiple pages and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    url = f\"https://books.toscrape.com/catalogue/page-{i}.html/\"\n",
    "    print(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Title   Price     Stock\n",
      "0             Slow States of Collapse: ...  £57.31  In stock\n",
      "1                    Reasons to Stay Alive  £26.41  In stock\n",
      "2              Private Paris (Private #10)  £47.61  In stock\n",
      "3          #HigherSelfie: Wake Up Your ...  £23.11  In stock\n",
      "4          Without Borders (Wanderlove #1)  £45.07  In stock\n",
      "5                         When We Collided  £31.77  In stock\n",
      "6                 We Love You, Charlie ...  £50.27  In stock\n",
      "7   Untitled Collection: Sabbath Poems ...  £14.27  In stock\n",
      "8             Unseen City: The Majesty ...  £44.18  In stock\n",
      "9                           Unicorn Tracks  £18.78  In stock\n",
      "10     Unbound: How Eight Technologies ...  £25.52  In stock\n",
      "11          Tsubasa: WoRLD CHRoNiCLE 2 ...  £16.28  In stock\n",
      "12               Throwing Rocks at the ...  £31.12  In stock\n",
      "13                         This One Summer  £19.49  In stock\n",
      "14                                  Thirst  £17.27  In stock\n",
      "15                The Torch Is Passed: ...  £19.09  In stock\n",
      "16           The Secret of Dreadwillow ...  £56.13  In stock\n",
      "17            The Pioneer Woman Cooks: ...  £56.41  In stock\n",
      "18                     The Past Never Ends  £56.50  In stock\n",
      "19              The Natural History of ...  £45.22  In stock\n"
     ]
    }
   ],
   "source": [
    "pages = np.arange(1,4)\n",
    "for page in pages:\n",
    "    url = f\"https://books.toscrape.com/catalogue/page-{page}.html\"\n",
    "\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "    title_list = []\n",
    "    price_list = []\n",
    "    stock_list = []\n",
    "\n",
    "    def Books() -> str:\n",
    "        div = soup.find_all(\"li\", class_ = \"col-xs-6 col-sm-4 col-md-3 col-lg-3\")\n",
    "        for title in div:\n",
    "            title_con = title.find(\"h3\")\n",
    "            for i in title_con:\n",
    "                # print(i.text, end=\"\")\n",
    "                title_list.append(i.text)\n",
    "\n",
    "        \n",
    "        for price_con in div:\n",
    "            price = price_con.find(\"p\", class_ = \"price_color\")\n",
    "            # print(price.text)\n",
    "            price_list.append(price.text)\n",
    "        \n",
    "        for stock_con in div:\n",
    "            stock = stock_con.find(\"p\", class_ = \"instock availability\")\n",
    "            # print(str(stock.text).strip())\n",
    "            s = str(stock.text).strip()\n",
    "            stock_list.append(s)\n",
    "        \n",
    "        return title_list, price_list, stock_list\n",
    "\n",
    "def data_storing():\n",
    "    b_fun = Books()\n",
    "    Books_Data = {\n",
    "        \"Title\": b_fun[0],\n",
    "        \"Price\": b_fun[1],\n",
    "        \"Stock\": b_fun[2]\n",
    "    }\n",
    "    Outcome = pd.DataFrame(Books_Data)\n",
    "    print(Outcome)\n",
    "    \n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(\"Data Fetiching Error\")\n",
    "else:\n",
    "    # print(Books())\n",
    "    data_storing()\n",
    "    # result = Books()\n",
    "    # print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Title   Price     Stock\n",
      "0       Slow States of Collapse: ...  £57.31  In stock\n",
      "1              Reasons to Stay Alive  £26.41  In stock\n",
      "2        Private Paris (Private #10)  £47.61  In stock\n",
      "3    #HigherSelfie: Wake Up Your ...  £23.11  In stock\n",
      "4    Without Borders (Wanderlove #1)  £45.07  In stock\n",
      "..                               ...     ...       ...\n",
      "175         The Torch Is Passed: ...  £19.09  In stock\n",
      "176    The Secret of Dreadwillow ...  £56.13  In stock\n",
      "177     The Pioneer Woman Cooks: ...  £56.41  In stock\n",
      "178              The Past Never Ends  £56.50  In stock\n",
      "179       The Natural History of ...  £45.22  In stock\n",
      "\n",
      "[180 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = data_storing()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = np.arange(1,15)\n",
    "title_list = []\n",
    "price_list = []\n",
    "stock_list = []\n",
    "for page in pages:\n",
    "\n",
    "    url = f\"https://books.toscrape.com/catalogue/page-{page}.html\"\n",
    "\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    \n",
    "    div = soup.find_all(\"li\", class_ = \"col-xs-6 col-sm-4 col-md-3 col-lg-3\")\n",
    "    for title in div:\n",
    "        title_con = title.find(\"h3\")\n",
    "        for i in title_con:\n",
    "            title_list.append(i.get_text())\n",
    "    for price_con in div:\n",
    "        price = price_con.find(\"p\", class_ = \"price_color\")\n",
    "        price_list.append(price.get_text())\n",
    "    for stock_con in div:\n",
    "        stock = stock_con.find(\"p\", class_ = \"instock availability\")\n",
    "        s = str(stock.get_text()).strip()\n",
    "        stock_list.append(s)\n",
    "    Books_Data = {\n",
    "        \"Title\": title_list,\n",
    "        \"Price\": price_list,\n",
    "        \"Stock\": stock_list\n",
    "    }\n",
    "Outcome = pd.DataFrame(Books_Data)\n",
    "# print(Outcome)\n",
    "\n",
    "storage_file = Outcome.to_csv(\"Storage.csv\", index=False)\n",
    "storage_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Overload: How to Unplug, ...</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Once Was a Time</td>\n",
       "      <td>£18.28</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Old School (Diary of ...</td>\n",
       "      <td>£11.83</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>No Dream Is Too ...</td>\n",
       "      <td>£21.95</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Naruto (3-in-1 Edition), Vol. ...</td>\n",
       "      <td>£38.39</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title   Price     Stock\n",
       "0                   A Light in the ...  £51.77  In stock\n",
       "1                   Tipping the Velvet  £53.74  In stock\n",
       "2                           Soumission  £50.10  In stock\n",
       "3                        Sharp Objects  £47.82  In stock\n",
       "4         Sapiens: A Brief History ...  £54.23  In stock\n",
       "..                                 ...     ...       ...\n",
       "275       Overload: How to Unplug, ...  £52.15  In stock\n",
       "276                    Once Was a Time  £18.28  In stock\n",
       "277           Old School (Diary of ...  £11.83  In stock\n",
       "278                No Dream Is Too ...  £21.95  In stock\n",
       "279  Naruto (3-in-1 Edition), Vol. ...  £38.39  In stock\n",
       "\n",
       "[280 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Storage.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetiching From Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://webscraper.io/test-sites/tables\"\n",
    "\n",
    "res = requests.get(url)\n",
    "if res.status_code != 200:\n",
    "    print(\"Data fetiching Error!\")\n",
    "else:\n",
    "    header = []\n",
    "    rows = []\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "    table = soup.find_all(\"table\", class_ = \"table table-bordered\")[1]\n",
    "    # tr = table.find_all(\"tr\")\n",
    "    # print(tr)\n",
    "    for i, row in enumerate(table.find_all(\"tr\")):\n",
    "        if i == 0:\n",
    "            headers = row.find_all(\"th\")\n",
    "            header = [x.text.strip() for x in headers]\n",
    "    \n",
    "        else:\n",
    "            rows.append([x.text.strip() for x in row.find_all(\"td\")])\n",
    "    # print(header)\n",
    "    # print(rows)\n",
    "\n",
    "    # Data_Frame = {\n",
    "    #     header[0]: [x for x in rows[0]],\n",
    "    #     header[1]: [x for x in rows[1]],\n",
    "    #     header[2]: [x for x in rows[2]],\n",
    "    # }\n",
    "    # table = pd.DataFrame(Data_Frame)\n",
    "    table = pd.DataFrame(rows, columns=header)\n",
    "    print(table.to_string(index=False))\n",
    "        # if i == 0:\n",
    "        #     header = [el.text.strip() for el in row.find(\"th\")]\n",
    "        # else:\n",
    "        #     rows.append([el.text.strip() for el in row.find(\"th\")])\n",
    "    # print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import datetime\n",
    "\n",
    "url = \"https://www.airbnb.com/s/Lahore--Pakistan/homes?place_id=ChIJ2QeB5YMEGTkRYiR-zGy-OsI&refinement_paths%5B%5D=%2Fhomes\"\n",
    "res = requests.get(url)\n",
    "\n",
    "titles = []\n",
    "locations = []\n",
    "room_details = []\n",
    "price_per_nights = []\n",
    "reviews = []\n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(\"Data fetiching Error!\")\n",
    "else:\n",
    "    # while True:\n",
    "    for i in range(1,14):\n",
    "        soup = BeautifulSoup(res.content, \"lxml\")\n",
    "        # print(res.status_code)\n",
    "        np = soup.find(\"a\", class_ = \"l1j9v1wn c1ytbx3a dir dir-ltr\").get(\"href\")\n",
    "        # print(np)\n",
    "        cnp = \"https://www.airbnb.com/\"+np\n",
    "        # print(cnp)\n",
    "        url = cnp\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "        title = soup.find_all(\"div\", class_ = \"t1jojoys dir dir-ltr\")\n",
    "        for i in title:\n",
    "            ti = i.get_text().strip()\n",
    "            # print(ti)\n",
    "            titles.append(ti)\n",
    "\n",
    "        location = soup.find_all(\"span\", class_ = \"t6mzqp7 dir dir-ltr\")\n",
    "        for i in location:\n",
    "            # print(i.get_text())\n",
    "            locations.append(i.get_text().strip())\n",
    "\n",
    "        room = soup.find_all(\"span\", class_ = [\"t6mzqp7 dir dir-ltr\"])\n",
    "        for i in room:\n",
    "            # print(i.get_text())\n",
    "            room_details.append(i.get_text().strip())\n",
    "\n",
    "        price = soup.find_all(\"div\", class_ = \"_1jo4hgw\")\n",
    "        for i in price:\n",
    "            # print(i.get_text())\n",
    "            price_per_nights.append(i.get_text().strip())\n",
    "\n",
    "        review = soup.find_all(\"span\", class_ = \"r1dxllyb dir dir-ltr\")\n",
    "        for i in review:\n",
    "            # print(i.get_text())\n",
    "            reviews.append(i.string.strip())\n",
    "\n",
    "    \n",
    "# print(len(titles))\n",
    "# print(len(locations))\n",
    "# print(len(room_details))\n",
    "# print(len(price_per_nights))\n",
    "# print(len(reviews))\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"Title\": [i for i in titles],\n",
    "    \"Location\": [i for i in locations],\n",
    "    \"Room_Details\": [i for i in room_details],\n",
    "    \"Price\": [i for i in price_per_nights],\n",
    "    # \"Reviews\": [i for i in reviews],\n",
    "})\n",
    "\n",
    "# data = pd.DataFrame({\n",
    "#     \"Title\": titles,\n",
    "#     \"Location\": locations,\n",
    "#     \"Room_Details\": room_details,\n",
    "#     \"Price\": price_per_nights,\n",
    "#     \"Reviews\": reviews,\n",
    "# })\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonds Data Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://hamariweb.com/finance/prizebonds/draw-94-of-1500-on-15-may-2023-held-in-lahore-did613.aspx\"\n",
    "res = requests.get(url)\n",
    "headers = [\"Data\"]\n",
    "rows = []\n",
    "res = requests.get(url)\n",
    "if res.status_code != 200:\n",
    "    print(\"Data fetiching Error!\")\n",
    "else:\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "    # td = soup.find_all(\"td\", {\"class\": \"third_td\"})\n",
    "    # for i in td:\n",
    "    #     print((str(i.text).strip()).zfill(6))\n",
    "    \n",
    "    # table = soup.find_all(\"table\", {\"class\": \"table\"})[1]\n",
    "    # td = table.find(\"td\", class_ = \"letter_space\")\n",
    "    # print(td.text)\n",
    "\n",
    "    # table = soup.find_all(\"table\", {\"class\": \"table\"})[2]\n",
    "    # for i in table.find_all(\"td\", class_ = \"letter_space\"):\n",
    "    #     print(i.text)\n",
    "\n",
    "\n",
    "\n",
    "# ////////////////////////////////////////////////////////\n",
    "\n",
    "    # td = soup.find_all(\"td\", {\"class\": \"third_td\"})\n",
    "    # for i in td:\n",
    "    #     print((str(i.text).strip()).zfill(6))\n",
    "    \n",
    "    table = soup.find_all(\"table\", {\"class\": \"table\"})[1]\n",
    "    td = table.find(\"td\", class_ = \"letter_space\")\n",
    "    # print(td.text)\n",
    "    rows.append(td.text.strip())\n",
    "\n",
    "\n",
    "    table = soup.find_all(\"table\", {\"class\": [\"table\",\"nbr_table\"]})\n",
    "    for i in table[2].find_all(\"td\", class_ = [\"letter_space\",\"third_td\"]):\n",
    "        # print((str(i.text).strip()).zfill(6))\n",
    "        rows.append(i.text.strip())\n",
    "    for i in table[3].find_all(\"td\", class_ = [\"letter_space\",\"third_td\"]):\n",
    "        # print((str(i.text).strip()).zfill(6))\n",
    "        rows.append(i.text.strip())\n",
    "# for x in rows:\n",
    "#     print(x)\n",
    "Data = pd.DataFrame({\n",
    "    headers[0]: [x for x in rows],\n",
    "})\n",
    "# Data.to_excel(\"C:\\\\Users\\\\Abdullah khan\\\\Desktop\\\\bonds\\\\15May_(1500)_3_list.xlsx\", index=False)\n",
    "W_bond_list = pd.read_excel(\"C:\\\\Desktop\\\\bonds\\\\15May_(1500)_3_list.xlsx\")\n",
    "O_bond_list = pd.read_excel(\"C:\\\\Desktop\\\\bonds\\\\Bonds_list.xlsx\")\n",
    "for i in O_bond_list.loc[:,[\"1500\"]]:\n",
    "    for j in O_bond_list[i].dropna().astype(int):\n",
    "        for x in W_bond_list[\"Data\"].astype(int):\n",
    "            if str(x).zfill(6) == str(j).zfill(6):\n",
    "                print(f\"{x} is present in list at column positon {i}\")\n",
    "    else:\n",
    "        print(f\"No value is present in the list {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
